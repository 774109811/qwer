mysql与redis 区别
=========================
| 1 | 2 | 3 |
| :----| :---- | :----: |
| 标题 | mysql |                                                    	redis |
| 开启事务 |	start transaction命令 |	                           multi命令 |
| 回滚事务 |	使用rollback命令可以回滚事务 |	                    不能回滚事务。但使用discard命令可以放弃事务queue中的sql |
| 提交事务 |	commit命令,即使遇到sql语法错误也会提交事务|	       exec命令,如果遇到sql语法错误会放弃事务中的sql |
| 悲观锁 |	使用select ... for update实现悲观锁|	               无 |
| 乐观锁 |	通常使用version或时间戳来实现乐观锁|	                 使用watch监控对象变化来实现乐观锁 |
| 原子性（Atomicity）	|        具备 |	                               具备 |
| 一致性（Consistency） |	具备 |	                               具备 |
| 隔离性（Isolation）  | 	具备 |	                               具备 |
| 持久性（Durability） |	具备 |	                               当redis服务器使用AOF持久化模式并appendfsync设置为always时具备|

mysql
-----------------------
* 1, 开启一个事务： 
* start transaction; 
* 也可以使用“begin; ” 
* 2，执行业务中需要保持逻辑一致性的多条语句； 
* insert .....; 
* delete ......; 
* update ......; 
* 事务中，其实只是对增删改进行控制，而查询语句，无所谓——因为查询不改变数据； 
* 3，判断是否出错并采取相应措施：
```
if （出错）{ 
rollback;//就是通常所说的“回滚”——就是啥也不做了，就是全部撤销； 
} 
else{ 
commit;//就是通常所说的“提交”——就是全都生效； 
}
```
* 1.mysql实现事务，是基于undo/redo日志
* 2.undo记录修改前状态，rollback基于undo日志实现
* 3.redo记录修改后的状态，commit基于redo日志实现
* 4.在mysql中无论是否开启事务，sql都会被立即执行并返回执行结果，只是事务开启后执行后的状态只是记录在redo日志，执行commit之后，数据才会被写入磁盘

redis 
----------------------
* Multi：标记事务的开始
* Exec：执行事务的commands队列
* Discard：结束事务，并清除commands队列
* 批量操作在发送 EXEC 命令前被放入队列缓存。
* 收到 EXEC 命令后进入事务执行，事务中任意命令执行失败，其余的命令依然被执行。
* 在事务执行过程，其他客户端提交的命令请求不会插入到事务执行命令序列中。

缓存不一致的问题
===================================  
### 1.先删缓存，再更新数据库
* 该方案会导致不一致的原因是。同时有一个请求A进行更新操作，另一个请求B进行查询操作。那么会出现如下情形: /b
* （1）请求A进行写操作，删除缓存
* （2）请求B查询发现缓存不存在
* （3）请求B去数据库查询得到旧值
* （4）请求B将旧值写入缓存
* （5）请求A将新值写入数据库
* 上述情况就会导致不一致的情形出现。而且，如果不采用给缓存设置过期时间策略，该数据永远都是脏数据。
* 那么，如何解决呢？采用延时双删策略
* 伪代码如下
```
public void write(String key,Object data){
		redis.delKey(key);
	    db.updateData(data);
	    Thread.sleep(1000);
	    redis.delKey(key);
	}
```
* 转化为中文描述就是
* （1）先淘汰缓存
* （2）再写数据库（这两步和原来一样）
* （3）休眠1秒，再次淘汰缓存
* 这么做，可以将1秒内所造成的缓存脏数据，再次删除。
* 那么，这个1秒怎么确定的，具体该休眠多久呢？
* 针对上面的情形，读者应该自行评估自己的项目的读数据业务逻辑的耗时。然后写数据的休眠时间则在读数据业务逻辑的耗时基础上，加几百ms即可。这么做的目的，就是确保读请求结束，写请求可以删除读请求造成的* 缓存脏数据。
* 如果你用了mysql的读写分离架构怎么办？
* ok，在这种情况下，造成数据不一致的原因如下，还是两个请求，一个请求A进行更新操作，另一个请求B进行查询操作。
* （1）请求A进行写操作，删除缓存
* （2）请求A将数据写入数据库了，
* （3）请求B查询缓存发现，缓存没有值
* （4）请求B去从库查询，这时，还没有完成主从同步，因此查询到的是旧值
* （5）请求B将旧值写入缓存
* （6）数据库完成主从同步，从库变为新值
* 上述情形，就是数据不一致的原因。还是使用双删延时策略。只是，睡眠时间修改为在主从同步的延时时间基础上，加几百ms。
* 采用这种同步淘汰策略，吞吐量降低怎么办？
* ok，那就将第二次删除作为异步的。自己起一个线程，异步删除。这样，写的请求就不用沉睡一段时间后了，再返回。这么做，加大吞吐量。
* 第二次删除,如果删除失败怎么办？
* 这是个非常好的问题，因为第二次删除失败，就会出现如下情形。还是有两个请求，一个请求A进行更新操作，另一个请求B进行查询操作，为了方便，假设是单库：
* （1）请求A进行写操作，删除缓存
* （2）请求B查询发现缓存不存在
* （3）请求B去数据库查询得到旧值
* （4）请求B将旧值写入缓存
* （5）请求A将新值写入数据库
* （6）请求A试图去删除请求B写入对缓存值，结果失败了。
* ok,这也就是说。如果第二次删除缓存失败，会再次出现缓存和数据库不一致的问题。
* 如何解决呢？
* 具体解决方案，且看博主对第(3)种更新策略的解析。

### (3)先更新数据库，再删缓存
* 首先，先说一下。老外提出了一个缓存更新套路，名为《Cache-Aside pattern》。其中就指出
* 失效：应用程序先从cache取数据，没有得到，则从数据库中取数据，成功后，放到缓存中。
* 命中：应用程序从cache中取数据，取到后返回。
* 更新：先把数据存到数据库中，成功后，再让缓存失效。
* 另外，知名社交网站facebook也在论文《Scaling Memcache at Facebook》中提出，他们用的也是先更新数据库，再删缓存的策略。
* 这种情况不存在并发问题么？
* 不是的。假设这会有两个请求，一个请求A做查询操作，一个请求B做更新操作，那么会有如下情形产生
* （1）缓存刚好失效
* （2）请求A查询数据库，得一个旧值
* （3）请求B将新值写入数据库
* （4）请求B删除缓存
* （5）请求A将查到的旧值写入缓存
* ok，如果发生上述情况，确实是会发生脏数据。
* 然而，发生这种情况的概率又有多少呢？
* 发生上述情况有一个先天性条件，就是步骤（3）的写数据库操作比步骤（2）的读数据库操作耗时更短，才有可能使得步骤（4）先于步骤（5）。可是，大家想想，数据库的读操作的速度远快于写操作的（不然做读写分* 离干嘛，做读写分离的意义就是因为读操作比较快，耗资源少），因此步骤（3）耗时比步骤（2）更短，这一情形很难出现。
* 假设，有人非要抬杠，有强迫症，一定要解决怎么办？
* 如何解决上述并发问题？
* 首先，给缓存设有效时间是一种方案。其次，采用策略（2）里给出的异步延时删除策略，保证读请求完成以后，再进行删除操作。
* 还有其他造成不一致的原因么？
* 有的，这也是缓存更新策略（2）和缓存更新策略（3）都存在的一个问题，如果删缓存失败了怎么办，那不是会有不一致的情况出现么。比如一个写数据请求，然后写入数据库了，删缓存失败了，这会就出现不一致的情* 况了。这也是缓存更新策略（2）里留下的最后一个疑问。
* 如何解决？
* 提供一个保障的重试机制即可，这里给出两套方案。
#### 方案一：
* 如下图所示
* image
* 流程如下所示
* （1）更新数据库数据；
* （2）缓存因为种种问题删除失败
* （3）将需要删除的key发送至消息队列
* （4）自己消费消息，获得需要删除的key
* （5）继续重试删除操作，直到成功
* 然而，该方案有一个缺点，对业务线代码造成大量的侵入。于是有了方案二，在方案二中，启动一个订阅程序去订阅数据库的binlog，获得需要操作的数据。在应用程序中，另起一段程序，获得这个订阅程序传来的信* * 息，进行删除缓存操作。
#### 方案二：
* image
* 流程如下图所示：
* （1）更新数据库数据
* （2）数据库会将操作信息写入binlog日志当中
* （3）订阅程序提取出所需要的数据以及key
* （4）另起一段非业务代码，获得该信息
* （5）尝试删除缓存操作，发现删除失败
* （6）将这些信息发送至消息队列
* （7）重新从消息队列中获得该数据，重试操作。

* 备注说明：上述的订阅binlog程序在mysql中有现成的中间件叫canal，可以完成订阅binlog日志的功能。至于oracle中，博主目前不知道有没有现成中间件可以使用。另外，重试机制，博主是采用的是消息队列的方* * 式。如果对一致性要求不是很高，直接在程序中另起一个线程，每隔一段时间去重试即可，这些大家可以灵活自由发挥，只是提供一个思路。

消息队列的问题
----------------------
## 重复消费问题
保证消息的唯一性,就算多次传输,不要让消息的多次消费带来影响;保证消费的幂等性,比如: 在写入消息队列的数据做唯一表示,消费消息时,根据唯一标识判断是否消费过
那么如何怎么来保证消息消费的幂等性呢？实际上我们只要保证多条相同的数据过来的时候只处理一条或者说多条处理和处理一条造成的结果相同即可，但是具体怎么做要根据业务需求来定，例如入库消息，先查一下消息是否已经入库啊或者说搞个唯一约束啊什么的，还有一些是天生保证幂等性就根本不用去管，例如redis就是天然幂等性。

还有一个问题，消费者消费消息的时候在某些场景下要放过消费不了的消息，遇到消费不了的消息通过日志记录一下或者搞个什么措施以后再来处理，但是一定要放过消息，因为在某些场景下例如spring-rabbitmq的默认回馈策略是出现异常就没有提交ack，导致了一直在重发那条消费异常的消息，而且一直还消费不了，这就尴尬了，后果你会懂的。

① 如果写入数据库就先根据主键查一下，如果已存在就不插入，update即可。

② 如果是写入Redis，那就没问题，反正每次都是set，天然幂等性。

③ 生产者每次写入消息可以加入一个全局唯一ID，类似订单ID，当消费时，现根据ID去Redis查一下之前是否被消费过，如果被消费过就不处理。

④ 基于数据库的唯一键来保证重复数据不会重复插入多条。因为有唯一键约束，再次插入重复数据只会报错。
## 消息丢失
### 生产者丢失
### 消息列表丢失
### 消费者丢失

#### 生产者丢失
* rabbitmq 提供transaction 和confirm模式来确保生产者不丢消息
使用RabbitMQ的事务功能，此时可以选择用 RabbitMQ 提供的事务功能，就是生产者发送数据之前开启 RabbitMQ 事务channel.txSelect，然后发送消息，如果消息没有成功被 RabbitMQ 接收到，那么生产者会收到异常报错，此时就可以回滚事务channel.txRollback，然后重试发送消息；如果收到了消息，那么可以提交事务channel.txCommit。但是问题是，RabbitMQ 事务机制（同步）一搞，基本上吞吐量会下来，因为太耗性能。

RabbitMQ 的消息别丢，可以开启confirm模式，在生产者那里设置开启confirm模式之后，你每次写的消息都会分配一个唯一的 id，然后如果写入了 RabbitMQ 中，RabbitMQ 会给你回传一个ack消息，告诉你说这个消息 ok 了。如果 RabbitMQ 没能处理这个消息，会回调你一个nack接口，告诉你这个消息接收失败，你可以重试。而且你可以结合这个机制自己在内存里维护每个消息 id 的状态，如果超过一定时间还没接收到这个消息的回调，那么你可以重发。
　　事务机制和cnofirm机制最大的不同在于，事务机制是同步的，你提交一个事务之后会阻塞在那儿，但是confirm机制是异步的，你发送个消息之后就可以发送下一个消息，然后那个消息RabbitMQ 接收了之后会异步回调你一个接口通知你这个消息接收到了。
　　所以一般在生产者这块避免数据丢失，都是用confirm机制的。
  
  
#### 消息列表丢失
　　就是 RabbitMQ 自己弄丢了数据，这个你必须开启 RabbitMQ 的持久化，就是消息写入之后会持久化到磁盘，哪怕是 RabbitMQ 自己挂了，恢复之后会自动读取之前存储的数据，一般数据不会丢。除非极其罕见的是，RabbitMQ 还没持久化，自己就挂了，可能导致少量数据丢失，但是这个概率较小。

　设置持久化有两个步骤：

　创建 queue 的时候将其设置为持久化
这样就可以保证 RabbitMQ 持久化 queue 的元数据，但是不会持久化 queue 里的数据。
发送消息的时候将消息的 deliveryMode 设置为 2
就是将消息设置为持久化的，此时 RabbitMQ 就会将消息持久化到磁盘上去。
　　必须要同时设置这两个持久化才行，RabbitMQ 哪怕是挂了，再次重启，也会从磁盘上重启恢复 queue，恢复这个 queue 里的数据。

　　　持久化可以跟生产者那边的confirm机制配合起来，只有消息被持久化到磁盘之后，才会通知生产者ack了，所以哪怕是在持久化到磁盘之前，RabbitMQ 挂了，数据丢了，生产者收不到ack，你也是可以自己重发的。

　　　注意，哪怕是你给 RabbitMQ 开启了持久化机制，也有一种可能，就是这个消息写到了 RabbitMQ 中，但是还没来得及持久化到磁盘上，结果不巧，此时 RabbitMQ 挂了，就会导致内存里的一点点数据丢失。
#### 消费者丢失
　这个时候得用 RabbitMQ 提供的ack机制，简单来说，就是你关闭 RabbitMQ 的自动ack，可以通过一个 api 来调用就行，然后每次你自己代码里确保处理完的时候，再在程序里ack一把。这样的话，如果你还没处理完，不就没有ack？那 RabbitMQ 就认为你还没处理完，这个时候 RabbitMQ 会把这个消费分配给别的 consumer 去处理，消息是不会丢的。
